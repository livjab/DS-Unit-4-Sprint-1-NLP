{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2476, 7)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "df = df.dropna()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df[['id', 'description']]\n",
    "y = df['category']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1980, 2), (496, 2), (1980,), (496,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljohnson/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:18: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2104    [sourced, whiskey,, moved, bourbon, barrels,, ...\n",
       "222     [bottled, commemorate, 150th, anniversary, can...\n",
       "281     [thomas, chen, introduced, canadian, rockies, ...\n",
       "2631    [mix, bourbon, quarter, casks, finished, pedro...\n",
       "732     [marriage, 13, 16, year, old, bourbons, honori...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.tokenizer import Tokenizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for doc in tokenizer.pipe(X_train['description']):\n",
    "    doc_tokens = []\n",
    "    for token in doc:\n",
    "        if (token.is_stop == False) and (token.is_punct == False):\n",
    "            doc_tokens.append(token.text.lower())\n",
    "    tokens.append(doc_tokens)\n",
    "    \n",
    "X_train['tokens'] = tokens\n",
    "X_train['tokens'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljohnson/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>$160.</th>\n",
       "      <th>$60</th>\n",
       "      <th>(375</th>\n",
       "      <th>(400</th>\n",
       "      <th>(5,000</th>\n",
       "      <th>(50</th>\n",
       "      <th>...</th>\n",
       "      <th>½</th>\n",
       "      <th>ìle</th>\n",
       "      <th>‘house’</th>\n",
       "      <th>‘rothes</th>\n",
       "      <th>‘the</th>\n",
       "      <th>“a</th>\n",
       "      <th>“aged</th>\n",
       "      <th>“ardbeg</th>\n",
       "      <th>“new”</th>\n",
       "      <th>€50</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    \\n  \\n\\n  \\n          $160.  $60  (375  (400  (5,000  (50  ...    ½  ìle  \\\n",
       "0  0.0   0.0    0.0  0.0    0.0  0.0   0.0   0.0     0.0  0.0  ...  0.0  0.0   \n",
       "1  0.0   0.0    0.0  0.0    0.0  0.0   0.0   0.0     0.0  0.0  ...  0.0  0.0   \n",
       "2  0.0   0.0    0.0  0.0    0.0  0.0   0.0   0.0     0.0  0.0  ...  0.0  0.0   \n",
       "3  0.0   0.0    0.0  0.0    0.0  0.0   0.0   0.0     0.0  0.0  ...  0.0  0.0   \n",
       "4  0.0   0.0    0.0  0.0    0.0  0.0   0.0   0.0     0.0  0.0  ...  0.0  0.0   \n",
       "\n",
       "   ‘house’  ‘rothes  ‘the   “a  “aged  “ardbeg  “new”  €50  \n",
       "0      0.0      0.0   0.0  0.0    0.0      0.0    0.0  0.0  \n",
       "1      0.0      0.0   0.0  0.0    0.0      0.0    0.0  0.0  \n",
       "2      0.0      0.0   0.0  0.0    0.0      0.0    0.0  0.0  \n",
       "3      0.0      0.0   0.0  0.0    0.0      0.0    0.0  0.0  \n",
       "4      0.0      0.0   0.0  0.0    0.0      0.0    0.0  0.0  \n",
       "\n",
       "[5 rows x 6000 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_docs = list(X_train['tokens'])\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=6000, tokenizer=lambda doc: doc, lowercase=False)\n",
    "\n",
    "dtm_train = tfidf.fit_transform(train_docs)\n",
    "\n",
    "# View Feature Matrix as DataFrame\n",
    "train_df = pd.DataFrame(dtm_train.todense(), columns = tfidf.get_feature_names())\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljohnson/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='lbfgs',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression \n",
    "\n",
    "classifier = LogisticRegression(solver='lbfgs')\n",
    "\n",
    "classifier.fit(dtm_train.todense(), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljohnson/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:15: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1000    [sounding, like, character, asterix, whisky,, ...\n",
       "2458    [pittsburgh, distillery’s, series, “whims”, on...\n",
       "1061    [youngest, manse, brae, triumvirate,, freshest...\n",
       "2737    [inchmurrin, enjoyed, higher, profile, early, ...\n",
       "1182    [2009,, brewers, bob, baxter, alan, hansen, ad...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for doc in tokenizer.pipe(X_test['description']):\n",
    "    doc_tokens = []\n",
    "    for token in doc:\n",
    "        if (token.is_stop == False) and (token.is_punct == False):\n",
    "            doc_tokens.append(token.text.lower())\n",
    "    tokens.append(doc_tokens)\n",
    "    \n",
    "X_test['tokens'] = tokens\n",
    "X_test['tokens'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljohnson/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>\"comfort\"</th>\n",
       "      <th>\"polished</th>\n",
       "      <th>\"sure</th>\n",
       "      <th>#2096</th>\n",
       "      <th>...</th>\n",
       "      <th>“ultra</th>\n",
       "      <th>“weight”</th>\n",
       "      <th>“whims”</th>\n",
       "      <th>“work</th>\n",
       "      <th>“young</th>\n",
       "      <th></th>\n",
       "      <th>€1,000</th>\n",
       "      <th>€21</th>\n",
       "      <th>€40</th>\n",
       "      <th>€42</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.164746</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.193724</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.122392</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 6000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         \\n  \\n\\n  \\n     \\n               \"comfort\"  \"polished  \"sure  #2096  \\\n",
       "0  0.000000   0.0    0.0    0.0  0.0  0.0        0.0        0.0    0.0    0.0   \n",
       "1  0.000000   0.0    0.0    0.0  0.0  0.0        0.0        0.0    0.0    0.0   \n",
       "2  0.000000   0.0    0.0    0.0  0.0  0.0        0.0        0.0    0.0    0.0   \n",
       "3  0.122392   0.0    0.0    0.0  0.0  0.0        0.0        0.0    0.0    0.0   \n",
       "4  0.000000   0.0    0.0    0.0  0.0  0.0        0.0        0.0    0.0    0.0   \n",
       "\n",
       "   ...  “ultra  “weight”   “whims”  “work  “young    \n",
       "  €1,000  €21       €40  \\\n",
       "0  ...     0.0       0.0  0.000000    0.0     0.0  0.0     0.0  0.0  0.000000   \n",
       "1  ...     0.0       0.0  0.164746    0.0     0.0  0.0     0.0  0.0  0.000000   \n",
       "2  ...     0.0       0.0  0.000000    0.0     0.0  0.0     0.0  0.0  0.193724   \n",
       "3  ...     0.0       0.0  0.000000    0.0     0.0  0.0     0.0  0.0  0.000000   \n",
       "4  ...     0.0       0.0  0.000000    0.0     0.0  0.0     0.0  0.0  0.000000   \n",
       "\n",
       "   €42  \n",
       "0  0.0  \n",
       "1  0.0  \n",
       "2  0.0  \n",
       "3  0.0  \n",
       "4  0.0  \n",
       "\n",
       "[5 rows x 6000 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_docs = list(X_test['tokens'])\n",
    "\n",
    "tfidf = TfidfVectorizer(stop_words='english', max_features=6000, tokenizer=lambda doc: doc, lowercase=False)\n",
    "\n",
    "dtm_test = tfidf.fit_transform(test_docs)\n",
    "\n",
    "# View Feature Matrix as DataFrame\n",
    "test_df = pd.DataFrame(dtm_test.todense(), columns = tfidf.get_feature_names())\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6169354838709677"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(dtm_test.todense(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/84/4e2cae6247f397f83d8adc5c2a2a0c5d7d790a14a4c7400ff6574586f589/xgboost-0.90.tar.gz (676kB)\n",
      "\u001b[K    100% |████████████████████████████████| 686kB 7.2MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy in /Users/ljohnson/anaconda3/lib/python3.7/site-packages (from xgboost) (1.16.2)\n",
      "Requirement already satisfied: scipy in /Users/ljohnson/anaconda3/lib/python3.7/site-packages (from xgboost) (1.2.1)\n",
      "Building wheels for collected packages: xgboost\n",
      "  Building wheel for xgboost (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /Users/ljohnson/Library/Caches/pip/wheels/e9/48/4d/de4187b5270dff71d3697c5a7857a1e2d9a0c63a28b3462eeb\n",
      "Successfully built xgboost\n",
      "Installing collected packages: xgboost\n",
      "Successfully installed xgboost-0.90\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5625"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_classifier = XGBClassifier(max_depth=7, n_jobs=-1)\n",
    "xgb_classifier.fit(dtm_train.todense(), y_train)\n",
    "xgb_classifier.score(dtm_test.todense(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljohnson/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/Users/ljohnson/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9233870967741935"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trying it the J.C. way\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "vect = TfidfVectorizer(stop_words='english', max_features=5000, tokenizer=lambda doc: doc, lowercase=False)\n",
    "sgdc = SGDClassifier()\n",
    "\n",
    "pipe = Pipeline([('vect', vect), ('clf', sgdc)])\n",
    "\n",
    "# Fit Pipeline\n",
    "pipe.fit(train_docs, y_train)\n",
    "# test pipeline\n",
    "pipe.score(test_docs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 9 candidates, totalling 45 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  45 out of  45 | elapsed:   16.2s finished\n",
      "/Users/ljohnson/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/Users/ljohnson/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:183: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If max_iter is set but tol is left unset, the default value for tol in 0.19 and 0.20 will be None (which is equivalent to -infinity, so it has no effect) but will change in 0.21 to 1e-3. Specify tol to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9254032258064516"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# with a grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {\n",
    "    'vect__max_df': (0.5, 0.75, 1.0),\n",
    "    'clf__max_iter':(20, 10, 100)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe,parameters, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(train_docs, y_train)\n",
    "grid_search.score(test_docs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets submit a prediction with the test data\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "\n",
    "X = test[['id', 'description']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((288, 6), (288, 2))"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape, X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ljohnson/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:12: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    [think, carnival, aromas—the, good, ones,, any...\n",
       "1    [blend, bourbons,, 6, 12, years, old;, rye, wh...\n",
       "2    [nose, focused, cereal,, hints, fresh, ripe, c...\n",
       "3    [swiss-based, chapter, 7, released, 19, year, ...\n",
       "4    [valkyrie, replaces, current, dark, origins, e...\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer = Tokenizer(nlp.vocab)\n",
    "\n",
    "tokens = []\n",
    "\n",
    "for doc in tokenizer.pipe(X['description']):\n",
    "    doc_tokens = []\n",
    "    for token in doc:\n",
    "        if (token.is_stop == False) and (token.is_punct == False):\n",
    "            doc_tokens.append(token.text.lower())\n",
    "    tokens.append(doc_tokens)\n",
    "    \n",
    "X['tokens'] = tokens\n",
    "X['tokens'].head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = list(X['tokens'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>955</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3532</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1390</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1024</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1902</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     id  category\n",
       "0   955         2\n",
       "1  3532         2\n",
       "2  1390         1\n",
       "3  1024         1\n",
       "4  1902         1"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#going to submit this prediction\n",
    "y_pred = grid_search.predict(docs)\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "submission = sample_submission.copy()\n",
    "submission['category'] = y_pred\n",
    "submission = submission.astype('int64')\n",
    "\n",
    "submission.to_csv('LJ-first-submission.csv', index=False)\n",
    "#read it back to make sure we have ints not floats\n",
    "submission = pd.read_csv(\"LJ-first-submission.csv\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a download=\"data.csv\" href=\"data:text/csv;base64,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\" target=\"_blank\">Download CSV file</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# download file\n",
    "\n",
    "from IPython.display import HTML\n",
    "import base64\n",
    "\n",
    "def create_download_link( df, title = \"Download CSV file\", filename = \"data.csv\"):  \n",
    "    csv = df.to_csv(index=False)\n",
    "    b64 = base64.b64encode(csv.encode())\n",
    "    payload = b64.decode()\n",
    "    html = '<a download=\"{filename}\" href=\"data:text/csv;base64,{payload}\" target=\"_blank\">{title}</a>'\n",
    "    html = html.format(payload=payload,title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "create_download_link(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trying out LSI\n",
    "\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "svd = TruncatedSVD(n_components=100, \n",
    "                   algorithm='randomized',\n",
    "                   n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSI\n",
    "\n",
    "lsi = Pipeline([('vect', vect), ('svd', svd)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 3 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:   10.1s finished\n",
      "/Users/ljohnson/anaconda3/lib/python3.7/site-packages/sklearn/feature_extraction/text.py:301: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['b', 'c', 'd', 'e', 'f', 'g', 'h', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/Users/ljohnson/anaconda3/lib/python3.7/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDClassifier in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9173387096774194"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Pipe\n",
    "\n",
    "pipe = Pipeline([('lsi', lsi), ('clf', sgdc)])\n",
    "\n",
    "params = {\n",
    "    'lsi__vect__max_df':(0.5, 0.75, 1.0)\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, params, cv=5, n_jobs=-1, verbose=1)\n",
    "\n",
    "grid_search.fit(train_docs, y_train)\n",
    "grid_search.score(test_docs, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
